{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_pipeline import DatasetsGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('settings/model_testing.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from data_pipeline.transform import TransformTesting, TransformTraining\n",
    "\n",
    "class DatasetFromCocoAnnotations(Dataset):\n",
    "\n",
    "    def __init__(self, coco: COCO, images_dir: str, \n",
    "                 transform: TransformTesting | TransformTraining) -> None:\n",
    "        super().__init__()\n",
    "        self.coco = coco\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.idx_to_img = {i: coco.loadImgs(ids=[img])[0]\n",
    "                           for i, img in enumerate(self.coco.imgs)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns a sample of the dataset. If no transform is passed, the sample is a dictionary with:\n",
    "            - image\n",
    "            - landmarks:\n",
    "                - id\n",
    "                - category_id\n",
    "                - center_point\n",
    "                - size\n",
    "                - bbox\n",
    "                - area\n",
    "                - image_id\n",
    "            - original_image_size\n",
    "        '''\n",
    "        current_image_id = self.idx_to_img[idx]['id']\n",
    "        img_name = os.path.join(self.images_dir, self.idx_to_img[idx]['file_name'])\n",
    "\n",
    "        image = read_image(img_name, mode=ImageReadMode.RGB)\n",
    "\n",
    "        # Add center point and size to annotations\n",
    "        annotations_for_image = self.coco.imgToAnns[current_image_id]\n",
    "        for a in annotations_for_image:\n",
    "            # a[\"bbox\"] is top [left x position, top left y position, width, height]\n",
    "            center_point = (a[\"bbox\"][0] + a[\"bbox\"][2]/2, \n",
    "                            a[\"bbox\"][1] + a[\"bbox\"][3]/2)\n",
    "            size = (a[\"bbox\"][2], \n",
    "                    a[\"bbox\"][3])\n",
    "            a['center_point'] = center_point\n",
    "            a['size'] = size\n",
    "            if 'iscrowd' in a: del a['iscrowd'] # Pointless to keep it\n",
    "\n",
    "        sample = {'image': image, \n",
    "                  \"landmarks\": annotations_for_image,\n",
    "                  \"img_name\": img_name}\n",
    "\n",
    "        if isinstance(self.transform, (TransformTraining, TransformTesting)):\n",
    "            return self.transform(sample)       # sample, transformed_landmarks, original_sample\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.77s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_base = COCO(config['paths']['train_base_annotations_path'])\n",
    "val_base = COCO(config['paths']['val_base_annotations_path'])\n",
    "test_base = COCO(config['paths']['test_base_annotations_path'])\n",
    "images_dir = config['paths']['images_dir']\n",
    "\n",
    "dataset_base_train, dataset_base_val, dataset_base_test = (\n",
    "    DatasetFromCocoAnnotations(train_base, images_dir, TransformTraining(\n",
    "        config,\n",
    "        base_classes=list(train_base.cats),\n",
    "        novel_classes=[]\n",
    "    )),\n",
    "    DatasetFromCocoAnnotations(val_base, images_dir, TransformTraining(\n",
    "        config,\n",
    "        base_classes=list(val_base.cats),\n",
    "        novel_classes=[]\n",
    "    )),\n",
    "    DatasetFromCocoAnnotations(test_base, images_dir, TransformTesting(\n",
    "        config,\n",
    "        base_classes=list(test_base.cats),\n",
    "        novel_classes=[]\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 17435/170553 [04:54<58:35, 43.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 17427: Unsupported color conversion request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 152107/170553 [45:17<06:14, 49.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 152104: Unsupported color conversion request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170553/170553 [50:28<00:00, 56.32it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset_base_train))):\n",
    "    try:\n",
    "        result = dataset_base_train[i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21216/21216 [11:51<00:00, 29.82it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset_base_val))):\n",
    "    try:\n",
    "        result = dataset_base_val[i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21543 [00:00<?, ?it/s]/home/volpepe/miniconda3/envs/ml4cv/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|██████████| 21543/21543 [10:34<00:00, 33.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset_base_test))):\n",
    "    try:\n",
    "        result = dataset_base_test[i]\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train_val_images/Aves/Pyrocephalus rubinus/225f3aadbddd26da2cf4cc87e74e8ed3.jpg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.path.join(dataset_base_train.images_dir, dataset_base_train.idx_to_img[17427]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/train_val_images/Aves/Sturnus vulgaris/17ce5d50647b3217a2e24ec523f81378.jpg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.path.join(dataset_base_train.images_dir, dataset_base_train.idx_to_img[152104]['file_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
